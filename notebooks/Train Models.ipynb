{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb97fbb0",
   "metadata": {},
   "source": [
    "# Train DISTILBERT MODELS\n",
    "Evaluate them on the hand-tagged sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507fb85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import emoji\n",
    "import transformers\n",
    "from datasets import load_dataset, Dataset, DatasetDict,  concatenate_datasets\n",
    "from transformers import Trainer, TrainingArguments, pipeline, EarlyStoppingCallback\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_recall_fscore_support, log_loss\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random as rn\n",
    "from scipy.special import softmax\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set seed\n",
    "seed_value = 0\n",
    "# 1. Set `python` built-in pseudo-random generator at a fixed value\n",
    "rn.seed(seed_value)\n",
    "# 2. Set `numpy` pseudo-random generator at a fixed value\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "MODEL_NAME = 'distilbert-base-uncased'\n",
    "BATCH_SIZE = 128\n",
    "EMOTION_NUM_LABELS = 7\n",
    "EMOTIONS = ['neutral','happy','sad','anger','disgust','surprise','fear']\n",
    "SEQ_LENGTH = 64\n",
    "CALLBACK =  [EarlyStoppingCallback(early_stopping_patience = 1)]\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], truncation=True, padding=\"max_length\", max_length=SEQ_LENGTH)\n",
    "\n",
    "if torch.cuda.is_available():       \n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "    print('Device name:', torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "#Emotion Test sets\n",
    "test_df = pd.read_parquet('../emtract/data/tagged_sample.parquet.snappy', columns=['text','label'])\n",
    "\n",
    "# Emojis + emoticons\n",
    "emoticons = pd.read_csv('../emtract/data/dictionaries/emoticons.csv').values\n",
    "EMOJI_EMOTICONS = list(emoji.UNICODE_EMOJI[\"en\"].keys()) + list(emoticons[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2de15ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = transformers.DistilBertTokenizerFast.from_pretrained(MODEL_NAME, model_max_length=SEQ_LENGTH)\n",
    "# Add emoji-emoticons \n",
    "num_added_toks = tokenizer.add_tokens(EMOJI_EMOTICONS)\n",
    "print('We have added', num_added_toks, 'tokens')\n",
    " # Notice: resize_token_embeddings expect to receive the full size of the new vocabulary, i.e., the length of the tokenizer.\n",
    "model = transformers.DistilBertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=EMOTION_NUM_LABELS).to(device)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9444241e",
   "metadata": {},
   "source": [
    "## Train Emotion Model on Emotion Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341a5e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df = pd.read_parquet('../emtract/data/emotion_sources.parquet.snappy', columns=['text','label'])\n",
    "twitter_df['label'] = twitter_df['label'].astype(int)\n",
    "dataset = Dataset.from_pandas(twitter_df)\n",
    "\n",
    "train_testvalid = dataset.train_test_split(test_size=0.2)\n",
    "\n",
    "# gather everyone if you want to have a single DatasetDict\n",
    "dataset = DatasetDict({\n",
    "    'train': train_testvalid['train'],\n",
    "    'valid': train_testvalid['test']})\n",
    "\n",
    "emotions_encoded = dataset.map(tokenize, batched=True, batch_size=None)\n",
    "emotions_encoded.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea38ddc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging_steps = len(emotions_encoded[\"train\"]) // BATCH_SIZE\n",
    "training_args = TrainingArguments(output_dir=\"results\",\n",
    "                                  num_train_epochs=8,\n",
    "                                  learning_rate=2e-5,\n",
    "                                  per_device_train_batch_size=BATCH_SIZE,\n",
    "                                  per_device_eval_batch_size=BATCH_SIZE*4,\n",
    "                                  load_best_model_at_end=True,\n",
    "                                  metric_for_best_model=\"loss\",\n",
    "                                  weight_decay=0.01,\n",
    "                                  evaluation_strategy=\"epoch\",\n",
    "                                  save_strategy=\"epoch\",\n",
    "                                  disable_tqdm=False)\n",
    "\n",
    "trainer = Trainer(model=model, args=training_args,\n",
    "                  compute_metrics=compute_metrics,\n",
    "                  train_dataset=emotions_encoded[\"train\"],\n",
    "                  eval_dataset=emotions_encoded[\"valid\"],\n",
    "                   callbacks= CALLBACK)\n",
    "trainer.train();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d65ff15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.mkdir(\"models\")\n",
    "model.save_pretrained('./models/emotion-twitter')\n",
    "tokenizer.save_pretrained('./models/emotion-twitter')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8757e3ad",
   "metadata": {},
   "source": [
    "# Transfer it to StockTwits Hand-Tagged\n",
    "Use 8k observations for training/validation and hold out 2k for testing. Test both models via performance on test set during five-fold CV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96538084",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Grab tokenizer\n",
    "tokenizer = transformers.DistilBertTokenizerFast.from_pretrained('./models/emotion-twitter', model_max_length=SEQ_LENGTH)\n",
    "\n",
    "# Define the K-fold Cross Validator\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state = 0)\n",
    "\n",
    "tw_model = transformers.DistilBertForSequenceClassification.from_pretrained(\"./models/emotion-twitter\", num_labels=EMOTION_NUM_LABELS).to(device)\n",
    "tw_perf, transfer_perf = pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "# K-fold Cross Validation model evaluation\n",
    "for fold, (train_ids, test_ids) in enumerate(kfold.split(test_df)):\n",
    "    model = transformers.DistilBertForSequenceClassification.from_pretrained(\"./models/emotion-twitter\", num_labels=EMOTION_NUM_LABELS).to(device)\n",
    "    train_data = Dataset.from_pandas(test_df.loc[train_ids[1000:]])\n",
    "    valid_data = Dataset.from_pandas(test_df.loc[train_ids[:1000]])\n",
    "    test_data = Dataset.from_pandas(test_df.loc[test_ids])\n",
    "    dataset = DatasetDict({'train': train_data, 'valid': valid_data, 'test': test_data})\n",
    "    emotions_encoded = dataset.map(tokenize, batched=True, batch_size=None)\n",
    "    emotions_encoded.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "    logging_steps = len(emotions_encoded[\"train\"]) // BATCH_SIZE\n",
    "    training_args = TrainingArguments(output_dir=\"results\",\n",
    "                                      num_train_epochs=8,\n",
    "                                      learning_rate=2e-5,\n",
    "                                      per_device_train_batch_size=BATCH_SIZE,\n",
    "                                      per_device_eval_batch_size=BATCH_SIZE*4,\n",
    "                                      load_best_model_at_end=True,\n",
    "                                      metric_for_best_model=\"loss\",\n",
    "                                      weight_decay=0.01,\n",
    "                                      evaluation_strategy=\"epoch\",\n",
    "                                      save_strategy=\"epoch\",\n",
    "                                      disable_tqdm=False)\n",
    "\n",
    "    trainer = Trainer(model=model, args=training_args,\n",
    "                      compute_metrics=compute_metrics,\n",
    "                      train_dataset=emotions_encoded[\"train\"],\n",
    "                      eval_dataset=emotions_encoded[\"valid\"],\n",
    "                       callbacks= CALLBACK)\n",
    "    trainer.train();\n",
    "    \n",
    "    if fold == 2:\n",
    "        model.save_pretrained('./models/emotion-transfer')\n",
    "        tokenizer.save_pretrained('./models/emotion-transfer')\n",
    "    \n",
    "    print(\"Performance via transfer learning...\")\n",
    "    preds_output_transfer = trainer.predict(emotions_encoded[\"test\"])\n",
    "    print(preds_output_transfer.metrics)\n",
    "    m = softmax(preds_output_transfer[0], axis=1)\n",
    "    print(classification_report(preds_output_transfer[1], np.argmax(m, axis=1), target_names=EMOTIONS))\n",
    "    \n",
    "    # STORE TEST SET PERFORMANCE\n",
    "    transfer_perf = transfer_perf.append(pd.DataFrame(preds_output_transfer.metrics, index = [0]))\n",
    "    \n",
    "    print(\"Performance via Twitter model learning...\")\n",
    "    trainer_tw = Trainer(model=tw_model, args=training_args,\n",
    "                  compute_metrics=compute_metrics,\n",
    "                  train_dataset=emotions_encoded[\"train\"],\n",
    "                  eval_dataset=emotions_encoded[\"valid\"],\n",
    "                   callbacks= CALLBACK)\n",
    "    preds_output_tw = trainer_tw.predict(emotions_encoded[\"test\"])\n",
    "    print(preds_output_tw.metrics)\n",
    "    m = softmax(preds_output_tw[0], axis=1)\n",
    "    print(classification_report(preds_output_tw[1], np.argmax(m, axis=1), target_names=EMOTIONS))\n",
    "    \n",
    "    tw_perf = tw_perf.append(pd.DataFrame(preds_output_tw.metrics, index = [0]))\n",
    "\n",
    "tw_perf.to_csv(\"models/tw_perf.csv\", index = False)\n",
    "transfer_perf.to_csv(\"models/transfer_perf.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3161f758",
   "metadata": {},
   "source": [
    "## Explain Emotion Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52072959",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "# load emotion models\n",
    "transfer_model = transformers.DistilBertForSequenceClassification.from_pretrained(\"./models/emotion-transfer\", num_labels=EMOTION_NUM_LABELS).to(device)\n",
    "tw_model = transformers.DistilBertForSequenceClassification.from_pretrained(\"./models/emotion-twitter\", num_labels=EMOTION_NUM_LABELS).to(device)\n",
    "\n",
    "# load tokenizer\n",
    "tokenizer = transformers.DistilBertTokenizerFast.from_pretrained('./models/emotion-twitter', model_max_length=SEQ_LENGTH)\n",
    "\n",
    "# build a pipeline object to do predictions\n",
    "pred_tw = transformers.pipeline(\"text-classification\", model=tw_model, tokenizer=tokenizer, device=0, return_all_scores=True)\n",
    "pred_transfer = transformers.pipeline(\"text-classification\", model=transfer_model, tokenizer=tokenizer, device=0, return_all_scores=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ead3708",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "# Extract shap values for both models\n",
    "\n",
    "# Twitter only model first\n",
    "explainer_tw = shap.Explainer(pred_tw)\n",
    "shap_values_tw = explainer_tw(test_df['text'])\n",
    "\n",
    "# Transferred model second\n",
    "explainer_transfer = shap.Explainer(pred_transfer)\n",
    "shap_values_transfer = explainer_transfer(test_df['text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
